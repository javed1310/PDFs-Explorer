# PDFs Explorer
# ğŸ“š Chat with Multiple PDFs using Local LLMs (Streamlit + LangChain)

A powerful and lightweight app that allows users to **upload multiple PDF documents** and **ask questions about their contents** using **local LLMs** like TinyLlama, Phi-3, Qwen, and more â€” all from a simple **Streamlit interface**.

---

## ğŸš€ Features

- âœ… Upload and query **multiple PDF documents**
- âœ… Select from a range of **open-source LLMs**
- âœ… Embedding-based semantic search using **FAISS**
- âœ… Retains **conversational memory** for contextual responses
- âœ… Runs **fully offline** â€” no external API needed
- âœ… Built with **Streamlit, LangChain, Hugging Face, and PyPDF2**

---

## ğŸ› ï¸ Tech Stack

| Tool / Library         | Purpose                                      |
|------------------------|----------------------------------------------|
| Streamlit              | Web Interface                                |
| LangChain              | Conversational memory + LLM chaining         |
| HuggingFace Transformers | LLMs and tokenizers                       |
| HuggingFace Embeddings | Text vectorization                           |
| FAISS                  | Fast vector-based similarity search          |
| PyPDF2                 | Extract text from PDFs                       |
| dotenv                 | Manage environment variables (optional)      |

---

## ğŸ“‚ Project Structure

2. Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
3. Set Environment Variables
Create a .env file if needed:

env
Copy
Edit
HUGGINGFACEHUB_API_TOKEN=your_token_here  # Optional
4. Run the App
bash
Copy
Edit
streamlit run app.py
ğŸ¤– Supported LLM Models

Model	Description
TinyLlama	Lightweight, fast-performing open model
DistilGPT2	Compact version of GPT2
Qwen2-0.5B	Alibaba's efficient instruction-tuned model
MobileLLM-125M	Metaâ€™s mobile-friendly language model
Phi-3-mini	Microsoftâ€™s compact, high-performance model
ğŸ’¡ How It Works
ğŸ“„ Upload one or more PDF documents

ğŸ¤– Choose an LLM model from the sidebar

âš™ï¸ Click â€œProcessâ€ to extract text and create vector store

ğŸ’¬ Ask any question about your documents

ğŸ§  Get intelligent, context-aware responses from your selected model

ğŸ“Œ Notes
Some models may require substantial memory â€” use smaller ones for better performance on low-spec devices.

Ensure you have Python 3.8+ and a stable environment.

Phi-3-mini uses optional quantization (BitsAndBytesConfig) for memory efficiency.

ğŸ“œ License
This project is licensed under the MIT License. See the LICENSE file for more details.

ğŸ™‹â€â™‚ï¸ Author
Javed Ahmad



Built with â¤ï¸ to make document understanding faster and easier.
